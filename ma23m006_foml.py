# -*- coding: utf-8 -*-
"""MA23M006_FOML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10RtH0384XouDkZm9a8te1RaYVGThJPFF

#In below codes , eta = step_length

(1) You are given a data-set in the file FMLA1Q1Data train.csv with 10000 points in
(R
2
, R) (Each row corresponds to a datapoint where the first 2 components are features
and the last component is the associated y value).


#i. Write a piece of code to obtain the least squares solution wML to the regression problem using the analytical solution.

Step1: create function to load data and add bias term and check shapes of X and y
"""

import numpy as np #importing numpy library
import matplotlib.pyplot as plt #importing matplotlib for ploting


def data_read(file_path): #function to load data
    X = [] # empty X for storing features
    y = [] #empty y for storing target
    with open(file_path, 'r') as file: #reading file
        for line in file:
            values = list(map(float, line.strip().split(','))) #splitting line with respect to comma
            X.append(values[:2])  #first two elements are features
            y.append(values[2])   #third element is the target value
    return np.array(X), np.array(y) # return in numpy form

#adding bias term (i.e column 1 ) in X
def bias(X):
    ones = np.ones((X.shape[0], 1))  #column of ones
    return np.hstack((ones, X))  #horizontally stack the ones with the original X

file_path = '/content/FMLA1Q1Data_train.csv'  #file path
X, y = data_read(file_path)           #loading data
X = bias(X)                #adding bias term

X #printing X

print("shape of X",X.shape) #shape of X
print("shape of y",y.shape) #shape of y

"""Step 2: Create function to find least squares solution wML = (X^T X)^(-1) X^T y and print the solution"""

def least_squares(X, y):
    X_T_X_inv = np.linalg.inv(X.T @ X)  #computing (X^T X)^(-1)
    wML = X_T_X_inv @ X.T @ y           #compute wML = (X^T X)^(-1) X^T y
    return wML

wML = least_squares(X, y)  #computing the least squares solution
print("Least squares solution wML:", wML)

"""###Ans (i) - Least squares solution wML: [9.89400832 1.76570568 3.5215898 ]

#ii. Code the gradient descent algorithm with suitable step size to solve the least squares algorithms and plot $∥wt − w_{ML}∥_2$ as a function of t. What do you observe?

##Solution
The gradient descent update rule is $$w_{t+1} = w_t - \alpha \nabla J(w_t)$$.

Step1: create a function to compute Gradient descent
"""

def gradient_descent(X, y, wML, eta, iterations):
    w = np.zeros(X.shape[1])  # Initialize w with zeros
    norm_diff = []  # To store the norm difference at each iteration

    for t in range(iterations):
        gradient = (1 / X.shape[0]) * (X.T @ (X @ w - y))  # Computing X^T (Xw - y)
        w = w - eta * gradient  # Updating weights
        norm_diff.append(np.linalg.norm(w - wML, 2))  # Computing norm difference

    return w, norm_diff

"""Step 2: Create a plotting function

"""

def plot_diff(norm_diff, eta_values):
    for i, eta in enumerate(eta_values):
        plt.plot(norm_diff[i], label=f'eta={eta}', marker='o', markersize=2)

    plt.xlabel('Iteration (t)')
    plt.ylabel('Norm ∥w_t − wML∥_2')
    plt.title('Convergence of Gradient Descent for Different Learning Rates')
    plt.grid(True)
    plt.legend()
    plt.show()

eta_values = [0.001, 0.01, 0.1]  # Different values of eta
iterations = 1000  # Number of iterations
norm_diff_all = []  # To store norm differences for each eta

for eta in eta_values:
    w_gd, norm_diff = gradient_descent(X, y, wML, eta, iterations)  # Compute gradient descent for each eta
    norm_diff_all.append(norm_diff)  # Store the norm difference

plot_diff(norm_diff_all, eta_values)  # Plot for all eta values



"""#Observations
#Gradient Descent is Converging to the Analytical Solution:

1. The difference between the gradient descent solution
$ w_t $ and the analytical least squares solution
$ w_{ML} $ is shrinking.
Since $ || w_t - w_{ML} ||_2^2 $ is getting closer to zeroas as no. of iteration increasing , it means the gradient descent is progressively updating the weights $ w_t$ to approach $ w_{ML} $
This shows that the gradient descent algorithm is working correctly and is finding the optimal solution that minimizes the cost function.



2. Depending on the step size of eta , the rate of convergence can vary. If the step size is too large, the algorithm might not converge, while if it is too small, convergence will be slow.

# Question 1 part(c):
Code the stochastic gradient descent algorithm using batch size of 100 and plot
$∥wt − w_{ML}∥_2$ as a function of t. What are your observations?

step 1: create a function to compute Stochastic Gradient Descent
"""

def stochastic_gradient_descent(X, y, wML, eta, batch_size, iterations):
    w = np.zeros(X.shape[1])  # Initial w = zeros
    norm_diff = []  # Norm difference at each iteration will be stored here

    for t in range(iterations):
        batch_indices = np.random.choice(X.shape[0], batch_size, replace=False)
        X_batch = X[batch_indices]
        y_batch = y[batch_indices]

        gradient = (1 / X_batch.shape[0]) * (X_batch.T @ (X_batch @ w - y_batch))  # Compute X_batch^T (X_batch w - y_batch)

        w = w - eta * gradient  # Update weights
        norm_diff.append(np.linalg.norm(w - wML, 2))  # Compute norm difference

    return w, norm_diff

"""Step 2: create function to plot"""

def plot_diff(norm_diff_all, eta_values):
    for i, eta in enumerate(eta_values):
        plt.plot(norm_diff_all[i], label=f'eta={eta}', marker='o', markersize=2)

    plt.xlabel('Iteration (t)')
    plt.ylabel('Norm ∥w_t − wML∥_2')
    plt.title('Convergence of Stochastic Gradient Descent for Different Learning Rates')
    plt.grid(True)
    plt.legend()
    plt.show()

eta_values = [0.001, 0.01, 0.1]  # Different values of eta
batch_size = 100   # Batch size for SGD
iterations = 1000  # Number of iterations
norm_diff_all = []  # To store norm differences for each eta

for eta in eta_values:
    w_sgd, norm_diff = stochastic_gradient_descent(X, y, wML, eta, batch_size, iterations)  # Compute SGD for each eta
    norm_diff_all.append(norm_diff)  # Store the norm difference

plot_diff(norm_diff_all, eta_values)  # Plot for all eta values

"""#Observations
1. Initially, the norm  $ || w_t - w_{ML} ||_2 $
 will decrease rapidly, indicating that the weights are getting closer to the optimal solution.
2. Due to the randomness introduced by stochastic gradient descent, the curve may not be as smooth as in full-batch gradient descent. It may oscillate a bit because of the random sampling of batches, but overall, it should still trend downwards.
3. Depending on the learning rate eta , the batch size, and the total number of iterations, you may observe faster convergence compared to full-batch gradient descent.

Notes:

1. 	Step Size (eta) :If the step size is too large, the algorithm may not converge. If it’s too small, convergence will be slow.
2. Batch Size: A smaller batch size introduces more stochasticity, which can lead to faster but noisier convergence. A larger batch size makes the gradient more accurate but slower to compute.

#iv. Code the gradient descent algorithm for ridge regression. Cross-validate for various choices of λ and plot the error in the validation set as a function of λ. For the best λ chosen, obtain wR. Compare the test error (for the test data in the file FMLA1Q1Data test.csv) of wR with wML. Which is better and why?

Step 1: create function to compute ridge_regression_gradient_descent
"""

def ridge_regression_gradient_descent(X, y, lambda_val, alpha=0.01, num_iters=1000):
    n, d = X.shape
    w = np.zeros(d)
    for t in range(num_iters):
        gradient = X.T @ (X @ w - y) / n + lambda_val * w
        w = w - alpha * gradient
    return w

"""Step 2: we will use k fold for cross validation"""

def k_fold_cross_validation(X, y, k, lambda_val, alpha=0.01, num_iters=1000):
    n = len(y)
    fold_size = n // k
    validation_errors = []

    for fold in range(k):
        X_train = np.vstack([X[:fold * fold_size], X[(fold + 1) * fold_size:]]) #split into training and test part
        y_train = np.hstack([y[:fold * fold_size], y[(fold + 1) * fold_size:]])
        X_val = X[fold * fold_size:(fold + 1) * fold_size]
        y_val = y[fold * fold_size:(fold + 1) * fold_size]
        w_ridge = ridge_regression_gradient_descent(X_train, y_train, lambda_val, alpha, num_iters)

        y_val_pred = X_val @ w_ridge
        validation_error = np.mean((y_val - y_val_pred) ** 2)
        validation_errors.append(validation_error)

    return np.mean(validation_errors)

y_train = y
X_train = X

lambdas = np.logspace(-5, 2, 50)    #different values of lambda
k = 5  #number of folds
validation_errors = []

for lambda_val in lambdas:
    error = k_fold_cross_validation(X_train, y_train, k, lambda_val)
    validation_errors.append(error)

#ploting validation error as a function of lambda
plt.plot(lambdas, validation_errors, color='red', marker='o', linestyle='-', markersize=2)

plt.xscale('log')
plt.xlabel('Lambda')
plt.ylabel('Validation Error')
plt.title('Validation Error vs Lambda (Ridge Regression)')
plt.show()

#finding the best lambda and obtain wR
best_lambda = lambdas[np.argmin(validation_errors)]
wR = ridge_regression_gradient_descent(X_train, y_train, best_lambda)
print("Best lambda:", best_lambda)
print("wR:", wR)

#computing wML analytically
wML = np.linalg.inv(X_train.T @ X_train) @ X_train.T @ y_train

file_path = '/content/FMLA1Q1Data_test.csv'  #file path
X_test, y_test = data_read(file_path)           #loading data
X_test = bias(X_test)                #adding bias term

# Compute test error for wR (Ridge Regression)
y_test_pred_ridge = X_test @ wR
#print("y_test_pred_ridge",y_test_pred_ridge)
test_error_ridge = np.mean((y_test - y_test_pred_ridge) ** 2)

# Compute test error for wML (Least Squares)
y_test_pred_ml = X_test @ wML
test_error_ml = np.mean((y_test - y_test_pred_ml) ** 2)

print("Test error for wR (Ridge):", test_error_ridge)
print("Test error for wML (Least Squares):", test_error_ml)

if test_error_ridge < test_error_ml:
    print("Ridge regression performs better.")
else:
    print("Least squares regression performs better.")

"""##Observations:
1. Ridge Regression tends to perform better when the features are highly collinear or the model suffers from overfitting, as the regularization term helps reduce variance.
2. Least Squares Regression (wML) might perform better if the data does not suffer from overfitting or multicollinearity.
3. As lambda  increases, Ridge Regression adds more regularization, shrinking the weights and potentially reducing overfitting.

## v. Assume that you would like to perform kernel regression on this dataset. Which Kernel would you choose and why? Code the Kernel regression algorithm and predict for the test data. Argue why/why not the kernel you have chosen is a better kernel than the standard least squares regression.

step 1: create function to compute rbf kernel
"""

def rbf_kernel(X1, X2, sigma=1.0):
    sq_dists = np.sum(X1**2, axis=1).reshape(-1, 1) + np.sum(X2**2, axis=1) - 2 * X1 @ X2.T     #omputing the pairwise squared Euclidean distance
    return np.exp(-sq_dists / (2 * sigma**2))      # Apply the RBF kernel formula

"""step 2: creting function to compute kernel_ridge_regression


"""

def kernel_ridge_regression(X_train, y_train, X_test, lambda_val=0.01, sigma=1.0):
    # Compute the kernel matrix for the training data
    K = rbf_kernel(X_train, X_train, sigma=sigma)

    # Compute the alpha values (Ridge regression coefficients)
    alpha = np.linalg.inv(K + lambda_val * np.eye(K.shape[0])) @ y_train

    # Compute the kernel matrix for test data (between test and train)
    K_test = rbf_kernel(X_test, X_train, sigma=sigma)

    # Predict the output for test data
    y_pred = K_test @ alpha

    return y_pred

"""step 3: calculating all values"""

X_train = X
y_train = y
# Kernel regression parameters
lambda_val = 0.003727593720314938  # Regularization term
sigma = 1.0        # Kernel bandwidth (hyperparameter)

# Predict the test data using kernel regression
y_pred_kernel = kernel_ridge_regression(X_train, y_train, X_test, lambda_val, sigma)

# Compute the test error for kernel regression
test_error_kernel = np.mean((y_test - y_pred_kernel) ** 2)

# Compute the test error for least squares regression (wML)
X_train_with_bias = np.hstack([np.ones((X_train.shape[0], 1)), X_train])

X_test_with_bias = np.hstack([np.ones((X_test.shape[0], 1)), X_test])


print("Test error for Kernel Regression (RBF):", test_error_kernel)
print("Test error for Least Squares (wML):", test_error_ml)

"""##Step 4: Interpretation and Comparison

Why the RBF Kernel is Better:

1. Nonlinear relationships: The RBF kernel captures nonlinear relationships between the features, which might be crucial if the dataset has patterns that cannot be modeled by a simple linear function.
2. Localized impact: Each data point influences only the nearby points due to the exponential decay in the RBF function. This makes the model robust to outliers or irrelevant data points, unlike linear least squares regression, where all points influence the model equally.



Conclusion:

If kernel regression with the RBF kernel gives a lower test error, it indicates that the relationship between features and target values is likely nonlinear, and the RBF kernel successfully captured these patterns.

If least squares regression performs better, it suggests that the dataset's underlying relationship is close to linear, and the added complexity of kernel methods may not be necessary.
"""

